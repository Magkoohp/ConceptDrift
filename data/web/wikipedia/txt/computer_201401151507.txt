****** Computer ******
A computer is a general purpose device that can be programmed to carry out a
set of arithmetic or logical operations. Since a sequence of operations can be
readily changed, the computer can solve more than one kind of problem.
Conventionally, a computer consists of at least one processing element,
typically a central_processing_unit (CPU) and some form of memory. The
processing element carries out arithmetic and logic operations, and a
sequencing and control unit that can change the order of operations based on
stored information. Peripheral devices allow information to be retrieved from
an external source, and the result of operations saved and retrieved.
In World_War_II, mechanical analog_computers were used for specialized military
applications. During this time the first electronic digital computers were
developed. Originally they were the size of a large room, consuming as much
power as several hundred modern personal_computers (PCs).[1]
Modern computers based on integrated_circuits are millions to billions of times
more capable than the early machines, and occupy a fraction of the space.[2]
Simple computers are small enough to fit into mobile_devices, and mobile
computers can be powered by small batteries. Personal computers in their
various forms are icons of the Information_Age and are what most people think
of as “computers.” However, the embedded_computers found in many devices from
MP3_players to fighter_aircraft and from toys to industrial_robots are the most
numerous.
***** Etymology *****
The first use of the word “computer” was recorded in 1613 in a book called “The
yong mans gleanings” by English writer Richard Braithwait I haue read the
truest computer of Times, and the best Arithmetician that euer breathed, and he
reduceth thy dayes into a short number. It referred to a person who carried out
calculations, or computations, and the word continued with the same meaning
until the middle of the 20th century. From the end of the 19th century the word
began to take on its more familiar meaning, a machine that carries out
computations.[3]
***** History *****
Main article: History_of_computing_hardware
Although rudimentary calculating devices first appeared in antiquity and
mechanical calculating aids were invented in the 17th century, the first
'computers' were conceived of in the 19th century, and only emerged in their
modern form in the 1940s.
**** First general-purpose computing device ****
A portion of Babbage's Difference_engine.
Charles_Babbage, an English mechanical engineer and polymath, originated the
concept of a programmable computer. Considered the "father_of_the_computer",[4]
he conceptualized and invented the first mechanical_computer in the early 19th
century. After working on his revolutionary difference_engine, designed to aid
in navigational calculations, in 1833 he realized that a much more general
design, an Analytical_Engine, was possible. The input of programs and data was
to be provided to the machine via punched_cards, a method being used at the
time to direct mechanical looms such as the Jacquard_loom. For output, the
machine would have a printer, a curve plotter and a bell. The machine would
also be able to punch numbers onto cards to be read in later. The Engine
incorporated an arithmetic_logic_unit, control_flow in the form of conditional
branching and loops, and integrated memory, making it the first design for a
general-purpose computer that could be described in modern terms as Turing-
complete.[5][6]
The machine was about a century ahead of its time. All the parts for his
machine had to be made by hand - this was a major problem for a device with
thousands of parts. Eventually, the project was dissolved with the decision of
the British_Government to cease funding. Babbage's failure to complete the
analytical engine can be chiefly attributed to difficulties not only of
politics and financing, but also to his desire to develop an increasingly
sophisticated computer and to move ahead faster than anyone else could follow.
Nevertheless his son, Henry Babbage, completed a simplified version of the
analytical engine's computing unit (the mill) in 1888. He gave a successful
demonstration of its use in computing tables in 1906.
**** Analog computers ****
Sir_William_Thomson's third tide-predicting machine design, 1879-81
During the first half of the 20th century, many scientific computing needs were
met by increasingly sophisticated analog_computers, which used a direct
mechanical or electrical model of the problem as a basis for computation.
However, these were not programmable and generally lacked the versatility and
accuracy of modern digital computers.[7]
The first modern analog computer was a tide-predicting_machine, invented by Sir
William_Thomson in 1872. The differential_analyser, a mechanical analog
computer designed to solve differential equations by integration using wheel-
and-disc mechanisms, was conceptualized in 1876 by James_Thomson, the brother
of the more famous Lord Kelvin.[8]
The art of mechanical analog computing reached its zenith with the differential
analyzer, built by H. L. Hazen and Vannevar_Bush at MIT starting in 1927. This
built on the mechanical integrators of James_Thomson and the torque amplifiers
invented by H. W. Nieman. A dozen of these devices were built before their
obsolescence became obvious.
**** The modern computer ****
Alan_Turing was the first to conceptualize the modern computer, a device that
became known as the Universal_Turing_machine.
The principle of the modern computer was first described by computer_scientist
Alan_Turing, who set out the idea in his seminal 1936 paper,[9] On Computable
Numbers. Turing reformulated Kurt_Gödel's 1931 results on the limits of proof
and computation, replacing Gödel's universal arithmetic-based formal language
with the formal and simple hypothetical devices that became known as Turing
machines. He proved that some such machine would be capable of performing any
conceivable mathematical computation if it were representable as an algorithm.
He went on to prove that there was no solution to the Entscheidungsproblem by
first showing that the halting_problem for Turing machines is undecidable: in
general, it is not possible to decide algorithmically whether a given Turing
machine will ever halt.
He also introduced the notion of a 'Universal Machine' (now known as a
Universal_Turing_machine), with the idea that such a machine could perform the
tasks of any other machine, or in other words, it is provably capable of
computing anything that is computable by executing a program stored on tape,
allowing the machine to be programmable. Von_Neumann acknowledged that the
central concept of the modern computer was due to this paper.[10] Turing
machines are to this day a central object of study in theory_of_computation.
Except for the limitations imposed by their finite memory stores, modern
computers are said to be Turing-complete, which is to say, they have algorithm
execution capability equivalent to a universal_Turing_machine.
*** Electromechanical computers ***
Replica of Zuse's Z3, the first fully automatic, digital (electromechanical)
computer.
Early digital computers were electromechanical - electric switches drove
mechanical relays to perform the calculation. These devices had a low operating
speed and were eventually superseded by much faster all-electric computers,
originally using vacuum_tubes. The Z2, created by German engineer Konrad_Zuse
in 1939, was one of the earliest examples of an electromechanical relay
computer.[11]
In 1941, Zuse followed his earlier machine up with the Z3, the world's first
working electromechanical programmable, fully automatic digital computer.[12]
[13] The Z3 was built with 2000 relays, implementing a 22 bit word_length that
operated at a clock_frequency of about 5–10 Hz.[14] Program code and data were
stored on punched film. It was quite similar to modern machines in some
respects, pioneering numerous advances such as floating_point_numbers.
Replacement of the hard-to-implement decimal system (used in Charles_Babbage's
earlier design) by the simpler binary system meant that Zuse's machines were
easier to build and potentially more reliable, given the technologies available
at that time.[15] The Z3 was probably a complete Turing_machine.
*** Electronic programmable computer ***
Purely electronic_circuit elements soon replaced their mechanical and
electromechanical equivalents, at the same time that digital calculation
replaced analog. The engineer Tommy_Flowers, working at the Post_Office
Research_Station in Dollis_Hill in the 1930s, began to explore the possible use
of electronics for the telephone_exchange. Experimental equipment that he built
in 1934 went into operation 5 years later, converting a portion of the
telephone_exchange network into an electronic data processing system, using
thousands of vacuum_tubes.[7] In the US, John Vincent Atanasoff and Clifford E.
Berry of Iowa State University developed and tested the Atanasoff–Berry
Computer (ABC) in 1942,[16] The first electronic digital calculating device.
[17] This design was also all-electronic and used about 300 vacuum tubes, with
capacitors fixed in a mechanically rotating drum for memory.[18]
Colossus was the first electronic digital programmable computing device, and
was used to break German ciphers during World War II.
During World War II, the British at Bletchley_Park achieved a number of
successes at breaking encrypted German military communications. The German
encryption machine, Enigma, was first attacked with the help of the electro-
mechanical bombes. To crack the more sophisticated German Lorenz_SZ_40/42
machine, used for high-level Army communications, Max_Newman and his colleagues
commissioned Flowers to build the Colossus.[18] He spent eleven months from
early February 1943 designing and building the first Colossus.[19] After a
functional test in December 1943, Colossus was shipped to Bletchley Park, where
it was delivered on 18 January 1944[20] and attacked its first message on 5
February.[18]
Colossus was the world's first electronic digital programmable computer.[7] It
used a large number of valves (vacuum tubes). It had paper-tape input and was
capable of being configured to perform a variety of boolean_logical operations
on its data, but it was not Turing-complete. Nine Mk II Colossi were built (The
Mk I was converted to a Mk II making ten machines in total). Colossus Mark I
contained 1500 thermionic valves (tubes), but Mark II with 2400 valves, was
both 5 times faster and simpler to operate than Mark 1, greatly speeding the
decoding process.[21][22]
ENIAC was the first Turing-complete device,and performed ballistics trajectory
calculations for the United_States_Army.
The US-built ENIAC (Electronic Numerical Integrator and Computer) was the first
electronic programmable computer built in the US. Although the ENIAC was
similar to the Colossus it was much faster and more flexible. It was
unambiguously a Turing-complete device and could compute any problem that would
fit into its memory. Like the Colossus, a "program" on the ENIAC was defined by
the states of its patch cables and switches, a far cry from the stored_program
electronic machines that came later. Once a program was written, it had to be
mechanically set into the machine with manual resetting of plugs and switches.
It combined the high speed of electronics with the ability to be programmed for
many complex problems. It could add or subtract 5000 times a second, a thousand
times faster than any other machine. It also had modules to multiply, divide,
and square root. High speed memory was limited to 20 words (about 80 bytes).
Built under the direction of John_Mauchly and J._Presper_Eckert at the
University of Pennsylvania, ENIAC's development and construction lasted from
1943 to full operation at the end of 1945. The machine was huge, weighing 30
tons, using 200 kilowatts of electric power and contained over 18,000 vacuum
tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and
inductors.[23]
*** Stored program computer ***
[Three_tall_racks_containing_electronic_circuit_boards]
A section of the Manchester_Small-Scale_Experimental_Machine, the first stored-
program computer.
Early computing machines had fixed programs. Changing its function required the
re-wiring and re-structuring of the machine.[18] With the proposal of the
stored-program computer this changed. A stored-program computer includes by
design an instruction_set and can store in memory a set of instructions (a
program) that details the computation. The theoretical basis for the stored-
program computer was laid by Alan_Turing in his 1936 paper. In 1945 Turing
joined the National_Physical_Laboratory and began work on developing an
electronic stored-program digital computer. His 1945 report ‘Proposed
Electronic Calculator’ was the first specification for such a device. John_von
Neumann at the University_of_Pennsylvania, also circulated his First_Draft_of_a
Report_on_the_EDVAC in 1945.[7]
Ferranti_Mark_1, c. 1951.
The Manchester Small-Scale Experimental Machine, nicknamed Baby, was the
world's first stored-program_computer. It was built at the Victoria_University
of_Manchester by Frederic_C._Williams, Tom_Kilburn and Geoff Tootill, and ran
its first program on 21 June 1948.[24] It was designed as a testbed for the
Williams_tube the first random-access digital storage device.[25] Although the
computer was considered "small and primitive" by the standards of its time, it
was the first working machine to contain all of the elements essential to a
modern electronic computer.[26] As soon as the SSEM had demonstrated the
feasibility of its design, a project was initiated at the university to develop
it into a more usable computer, the Manchester_Mark_1.
The Mark 1 in turn quickly became the prototype for the Ferranti_Mark_1, the
world's first commercially available general-purpose computer.[27] Built by
Ferranti, it was delivered to the University_of_Manchester in February 1951. At
least seven of these later machines were delivered between 1953 and 1957, one
of them to Shell labs in Amsterdam.[28] In October 1947, the directors of
British catering company J._Lyons_&amp;_Company decided to take an active role
in promoting the commercial development of computers. The LEO_I computer became
operational in April 1951 [29] and ran the world's first regular routine office
computer job.
**** Transistor computers ****
A bipolar_junction_transistor
The bipolar transistor was invented in 1947. From 1955 onwards transistors
replaced vacuum_tubes in computer designs, giving rise to the "second
generation" of computers. Compared to vacuum tubes, transistors have many
advantages: they are smaller, and require less power than vacuum tubes, so give
off less heat. Silicon junction transistors were much more reliable than vacuum
tubes and had longer, indefinite, service life. Transistorized computers could
contain tens of thousands of binary logic circuits in a relatively compact
space.
At the University_of_Manchester, a team under the leadership of Tom_Kilburn
designed and built a machine using the newly developed transistors instead of
valves.[30] Their first transistorised_computer and the first in the world, was
operational_by_1953, and a second version was completed there in April 1955.
However, the machine did make use of valves to generate its 125 kHz clock
waveforms and in the circuitry to read and write on its magnetic drum_memory,
so it was not the first completely transistorized computer. That distinction
goes to the Harwell_CADET of 1955,[31] built by the electronics division of the
Atomic_Energy_Research_Establishment at Harwell.[32][33]
**** The integrated circuit ****
The next great advance in computing power came with the advent of the
integrated_circuit. The idea of the integrated circuit was first conceived by a
radar scientist working for the Royal_Radar_Establishment of the Ministry_of
Defence, Geoffrey_W.A._Dummer. Dummer presented the first public description of
an integrated circuit at the Symposium on Progress in Quality Electronic
Components in Washington, D.C. on 7 May 1952.[34]
Jack_Kilby's original integrated circuit.
The first practical ICs were invented by Jack_Kilby at Texas_Instruments and
Robert_Noyce at Fairchild_Semiconductor.[35] Kilby recorded his initial ideas
concerning the integrated circuit in July 1958, successfully demonstrating the
first working integrated example on 12 September 1958.[36] In his patent
application of 6 February 1959, Kilby described his new device as “a body of
semiconductor material ... wherein all the components of the electronic circuit
are completely integrated.”[37] Noyce also came up with his own idea of an
integrated circuit half a year later than Kilby.[38] His chip solved many
practical problems that Kilby's had not. Produced at Fairchild Semiconductor,
it was made of silicon, whereas Kilby's chip was made of germanium.
This new development heralded an explosion in the commercial and personal use
of computers and led to the invention of the microprocessor. While the subject
of exactly which device was the first microprocessor is contentious, partly due
to lack of agreement on the exact definition of the term "microprocessor", it
is largely undisputed that the first single-chip microprocessor was the Intel
4004,[39] designed and realized by Ted_Hoff, Federico_Faggin, and Stanley Mazor
at Intel.[40]
***** Programs *****
The defining feature of modern computers which distinguishes them from all
other machines is that they can be programmed. That is to say that some type of
instructions (the program) can be given to the computer, and it will process
them. Modern computers based on the von_Neumann_architecture often have machine
code in the form of an imperative_programming_language.
In practical terms, a computer program may be just a few instructions or extend
to many millions of instructions, as do the programs for word_processors and
web_browsers for example. A typical modern computer can execute billions of
instructions per second (gigaflops) and rarely makes a mistake over many years
of operation. Large computer programs consisting of several million
instructions may take teams of programmers years to write, and due to the
complexity of the task almost certainly contain errors.
**** Stored program architecture ****
Main articles: Computer_program and Computer_programming
Replica of the Small-Scale Experimental Machine (SSEM), the world's first
stored-program_computer, at the Museum_of_Science_and_Industry in Manchester,
England
This section applies to most common RAM_machine-based computers.
In most cases, computer instructions are simple: add one number to another,
move some data from one location to another, send a message to some external
device, etc. These instructions are read from the computer's memory and are
generally carried out (executed) in the order they were given. However, there
are usually specialized instructions to tell the computer to jump ahead or
backwards to some other place in the program and to carry on executing from
there. These are called “jump” instructions (or branches). Furthermore, jump
instructions may be made to happen conditionally so that different sequences of
instructions may be used depending on the result of some previous calculation
or some external event. Many computers directly support subroutines by
providing a type of jump that “remembers” the location it jumped from and
another instruction to return to the instruction following that jump
instruction.
Program execution might be likened to reading a book. While a person will
normally read each word and line in sequence, they may at times jump back to an
earlier place in the text or skip sections that are not of interest. Similarly,
a computer may sometimes go back and repeat the instructions in some section of
the program over and over again until some internal condition is met. This is
called the flow_of_control within the program and it is what allows the
computer to perform tasks repeatedly without human intervention.
Comparatively, a person using a pocket calculator can perform a basic
arithmetic operation such as adding two numbers with just a few button presses.
But to add together all of the numbers from 1 to 1,000 would take thousands of
button presses and a lot of time, with a near certainty of making a mistake. On
the other hand, a computer may be programmed to do this with just a few simple
instructions. For example:
      mov No. 0, sum     ; set sum to 0
      mov No. 1, num     ; set num to 1
loop: add num, sum    ; add num to sum
      add No. 1, num     ; add 1 to num
      cmp num, #1000  ; compare num to 1000
      ble loop        ; if num <= 1000, go back to 'loop'
      halt            ; end of program. stop running
Once told to run this program, the computer will perform the repetitive
addition task without further human intervention. It will almost never make a
mistake and a modern PC can complete the task in about a millionth of a second.
[41]
**** Bugs ****
Main article: Software_bug
The actual first computer bug, a moth found trapped on a relay of the Harvard
Mark II computer
Errors in computer programs are called “bugs.” They may be benign and not
affect the usefulness of the program, or have only subtle effects. But in some
cases, they may cause the program or the entire system to “hang,” becoming
unresponsive to input such as mouse clicks or keystrokes, to completely fail,
or to crash. Otherwise benign bugs may sometimes be harnessed for malicious
intent by an unscrupulous user writing an exploit, code designed to take
advantage of a bug and disrupt a computer's proper execution. Bugs are usually
not the fault of the computer. Since computers merely execute the instructions
they are given, bugs are nearly always the result of programmer error or an
oversight made in the program's design.[42]
Admiral Grace_Hopper, an American computer scientist and developer of the first
compiler, is credited for having first used the term “bugs” in computing after
a dead moth was found shorting a relay in the Harvard_Mark_II computer in
September 1947.[43]
**** Machine code ****
In most computers, individual instructions are stored as machine_code with each
instruction being given a unique number (its operation code or opcode for
short). The command to add two numbers together would have one opcode; the
command to multiply them would have a different opcode, and so on. The simplest
computers are able to perform any of a handful of different instructions; the
more complex computers have several hundred to choose from, each with a unique
numerical code. Since the computer's memory is able to store numbers, it can
also store the instruction codes. This leads to the important fact that entire
programs (which are just lists of these instructions) can be represented as
lists of numbers and can themselves be manipulated inside the computer in the
same way as numeric data. The fundamental concept of storing programs in the
computer's memory alongside the data they operate on is the crux of the von
Neumann, or stored program, architecture. In some cases, a computer might store
some or all of its program in memory that is kept separate from the data it
operates on. This is called the Harvard_architecture after the Harvard_Mark_I
computer. Modern von Neumann computers display some traits of the Harvard
architecture in their designs, such as in CPU_caches.
While it is possible to write computer programs as long lists of numbers
(machine_language) and while this technique was used with many early computers,
[44] it is extremely tedious and potentially error-prone to do so in practice,
especially for complicated programs. Instead, each basic instruction can be
given a short name that is indicative of its function and easy to remember – a
mnemonic such as ADD, SUB, MULT or JUMP. These mnemonics are collectively known
as a computer's assembly_language. Converting programs written in assembly
language into something the computer can actually understand (machine language)
is usually done by a computer program called an assembler.
A 1970s punched_card containing one line from a FORTRAN program. The card
reads: “Z(1) = Y + W(1)” and is labeled “PROJ039” for identification purposes.
**** Programming language ****
Main article: Programming_language
Programming languages provide various ways of specifying programs for computers
to run. Unlike natural_languages, programming languages are designed to permit
no ambiguity and to be concise. They are purely written languages and are often
difficult to read aloud. They are generally either translated into machine_code
by a compiler or an assembler before being run, or translated directly at run
time by an interpreter. Sometimes programs are executed by a hybrid method of
the two techniques.
*** Low-level languages ***
Main article: Low-level_programming_language
Machine languages and the assembly languages that represent them (collectively
termed low-level programming languages) tend to be unique to a particular type
of computer. For instance, an ARM_architecture computer (such as may be found
in a PDA or a hand-held_videogame) cannot understand the machine language of an
Intel_Pentium or the AMD_Athlon_64 computer that might be in a PC.[45]
*** Higher-level languages ***
Main article: High-level_programming_language
Though considerably easier than in machine language, writing long programs in
assembly language is often difficult and is also error prone. Therefore, most
practical programs are written in more abstract high-level_programming
languages that are able to express the needs of the programmer more
conveniently (and thereby help reduce programmer error). High level languages
are usually “compiled” into machine language (or sometimes into assembly
language and then into machine language) using another computer program called
a compiler.[46] High level languages are less related to the workings of the
target computer than assembly language, and more related to the language and
structure of the problem(s) to be solved by the final program. It is therefore
often possible to use different compilers to translate the same high level
language program into the machine language of many different types of computer.
This is part of the means by which software like video games may be made
available for different computer architectures such as personal computers and
various video_game_consoles.
**** Program design ****
                        This section does not cite any references_or_sources.
[Question_book-new.svg] Please help improve this section by adding_citations_to
                        reliable_sources. Unsourced material may be challenged
                        and removed. (July 2012)
Program design of small programs is relatively simple and involves the analysis
of the problem, collection of inputs, using the programming constructs within
languages, devising or using established procedures and algorithms, providing
data for output devices and solutions to the problem as applicable. As problems
become larger and more complex, features such as subprograms, modules, formal
documentation, and new paradigms such as object-oriented programming are
encountered. Large programs involving thousands of line of code and more
require formal software methodologies. The task of developing large software
systems presents a significant intellectual challenge. Producing software with
an acceptably high reliability within a predictable schedule and budget has
historically been difficult; the academic and professional discipline of
software_engineering concentrates specifically on this challenge.
***** Components *****
Main articles: Central_processing_unit and Microprocessor
[File:Computer Components.webm] 
Video demonstrating the standard components of a "slimline" computer
A general purpose computer has four main components: the arithmetic_logic_unit
(ALU), the control_unit, the memory, and the input and output devices
(collectively termed I/O). These parts are interconnected by buses, often made
of groups of wires.
Inside each of these parts are thousands to trillions of small electrical
circuits which can be turned off or on by means of an electronic_switch. Each
circuit represents a bit (binary digit) of information so that when the circuit
is on it represents a “1”, and when off it represents a “0” (in positive logic
representation). The circuits are arranged in logic_gates so that one or more
of the circuits may control the state of one or more of the other circuits.
The control unit, ALU, registers, and basic I/O (and often other hardware
closely linked with these) are collectively known as a central_processing_unit
(CPU). Early CPUs were composed of many separate components but since the mid-
1970s CPUs have typically been constructed on a single integrated_circuit
called a microprocessor.
**** Control unit ****
Main articles: CPU_design and Control_unit
Diagram showing how a particular MIPS_architecture instruction would be decoded
by the control system
The control unit (often called a control system or central controller) manages
the computer's various components; it reads and interprets (decodes) the
program instructions, transforming them into a series of control signals which
activate other parts of the computer.[47] Control systems in advanced computers
may change the order of some instructions so as to improve performance.
A key component common to all CPUs is the program_counter, a special memory
cell (a register) that keeps track of which location in memory the next
instruction is to be read from.[48]
The control system's function is as follows—note that this is a simplified
description, and some of these steps may be performed concurrently or in a
different order depending on the type of CPU:
   1. Read the code for the next instruction from the cell indicated by the
      program counter.
   2. Decode the numerical code for the instruction into a set of commands or
      signals for each of the other systems.
   3. Increment the program counter so it points to the next instruction.
   4. Read whatever data the instruction requires from cells in memory (or
      perhaps from an input device). The location of this required data is
      typically stored within the instruction code.
   5. Provide the necessary data to an ALU or register.
   6. If the instruction requires an ALU or specialized hardware to complete,
      instruct the hardware to perform the requested operation.
   7. Write the result from the ALU back to a memory location or to a register
      or perhaps an output device.
   8. Jump back to step (1).
Since the program counter is (conceptually) just another set of memory cells,
it can be changed by calculations done in the ALU. Adding 100 to the program
counter would cause the next instruction to be read from a place 100 locations
further down the program. Instructions that modify the program counter are
often known as “jumps” and allow for loops (instructions that are repeated by
the computer) and often conditional instruction execution (both examples of
control_flow).
The sequence of operations that the control unit goes through to process an
instruction is in itself like a short computer program, and indeed, in some
more complex CPU designs, there is another yet smaller computer called a
microsequencer, which runs a microcode program that causes all of these events
to happen.
**** Arithmetic logic unit (ALU) ****
Main article: Arithmetic_logic_unit
The ALU is capable of performing two classes of operations: arithmetic and
logic.[49]
The set of arithmetic operations that a particular ALU supports may be limited
to addition and subtraction, or might include multiplication, division,
trigonometry functions such as sine, cosine, etc., and square_roots. Some can
only operate on whole numbers (integers) whilst others use floating_point to
represent real_numbers, albeit with limited precision. However, any computer
that is capable of performing just the simplest operations can be programmed to
break down the more complex operations into simple steps that it can perform.
Therefore, any computer can be programmed to perform any arithmetic
operation—although it will take more time to do so if its ALU does not directly
support the operation. An ALU may also compare numbers and return boolean_truth
values (true or false) depending on whether one is equal to, greater than or
less than the other (“is 64 greater than 65?”).
Logic operations involve Boolean_logic: AND, OR, XOR and NOT. These can be
useful for creating complicated conditional_statements and processing boolean
logic.
Superscalar computers may contain multiple ALUs, allowing them to process
several instructions simultaneously.[50] Graphics_processors and computers with
SIMD and MIMD features often contain ALUs that can perform arithmetic on
vectors and matrices.
**** Memory ****
Main article: Computer_data_storage
Magnetic_core_memory was the computer memory of choice throughout the 1960s,
until it was replaced by semiconductor memory.
A computer's memory can be viewed as a list of cells into which numbers can be
placed or read. Each cell has a numbered “address” and can store a single
number. The computer can be instructed to “put the number 123 into the cell
numbered 1357” or to “add the number that is in cell 1357 to the number that is
in cell 2468 and put the answer into cell 1595.” The information stored in
memory may represent practically anything. Letters, numbers, even computer
instructions can be placed into memory with equal ease. Since the CPU does not
differentiate between different types of information, it is the software's
responsibility to give significance to what the memory sees as nothing but a
series of numbers.
In almost all modern computers, each memory cell is set up to store binary
numbers in groups of eight bits (called a byte). Each byte is able to represent
256 different numbers (2^8 = 256); either from 0 to 255 or −128 to +127. To
store larger numbers, several consecutive bytes may be used (typically, two,
four or eight). When negative numbers are required, they are usually stored in
two's_complement notation. Other arrangements are possible, but are usually not
seen outside of specialized applications or historical contexts. A computer can
store any kind of information in memory if it can be represented numerically.
Modern computers have billions or even trillions of bytes of memory.
The CPU contains a special set of memory cells called registers that can be
read and written to much more rapidly than the main memory area. There are
typically between two and one hundred registers depending on the type of CPU.
Registers are used for the most frequently needed data items to avoid having to
access main memory every time data is needed. As data is constantly being
worked on, reducing the need to access main memory (which is often slow
compared to the ALU and control units) greatly increases the computer's speed.
Computer main memory comes in two principal varieties: random-access_memory or
RAM and read-only_memory or ROM. RAM can be read and written to anytime the CPU
commands it, but ROM is preloaded with data and software that never changes,
therefore the CPU can only read from it. ROM is typically used to store the
computer's initial start-up instructions. In general, the contents of RAM are
erased when the power to the computer is turned off, but ROM retains its data
indefinitely. In a PC, the ROM contains a specialized program called the BIOS
that orchestrates loading the computer's operating_system from the hard disk
drive into RAM whenever the computer is turned on or reset. In embedded
computers, which frequently do not have disk drives, all of the required
software may be stored in ROM. Software stored in ROM is often called firmware,
because it is notionally more like hardware than software. Flash_memory blurs
the distinction between ROM and RAM, as it retains its data when turned off but
is also rewritable. It is typically much slower than conventional ROM and RAM
however, so its use is restricted to applications where high speed is
unnecessary.[51]
In more sophisticated computers there may be one or more RAM cache_memories,
which are slower than registers but faster than main memory. Generally
computers with this sort of cache are designed to move frequently needed data
into the cache automatically, often without the need for any intervention on
the programmer's part.
**** Input/output (I/O) ****
Main article: Input/output
Hard_disk_drives are common storage devices used with computers.
I/O is the means by which a computer exchanges information with the outside
world.[52] Devices that provide input or output to the computer are called
peripherals.[53] On a typical personal computer, peripherals include input
devices like the keyboard and mouse, and output devices such as the display and
printer. Hard_disk_drives, floppy_disk_drives and optical_disc_drives serve as
both input and output devices. Computer_networking is another form of I/O.
I/O devices are often complex computers in their own right, with their own CPU
and memory. A graphics_processing_unit might contain fifty or more tiny
computers that perform the calculations necessary to display 3D_graphics.
[citation_needed] Modern desktop_computers contain many smaller computers that
assist the main CPU in performing I/O.
**** Multitasking ****
Main article: Computer_multitasking
While a computer may be viewed as running one gigantic program stored in its
main memory, in some systems it is necessary to give the appearance of running
several programs simultaneously. This is achieved by multitasking i.e. having
the computer switch rapidly between running each program in turn.[54]
One means by which this is done is with a special signal called an interrupt,
which can periodically cause the computer to stop executing instructions where
it was and do something else instead. By remembering where it was executing
prior to the interrupt, the computer can return to that task later. If several
programs are running “at the same time,” then the interrupt generator might be
causing several hundred interrupts per second, causing a program switch each
time. Since modern computers typically execute instructions several orders of
magnitude faster than human perception, it may appear that many programs are
running at the same time even though only one is ever executing in any given
instant. This method of multitasking is sometimes termed “time-sharing” since
each program is allocated a “slice” of time in turn.[55]
Before the era of cheap computers, the principal use for multitasking was to
allow many people to share the same computer.
Seemingly, multitasking would cause a computer that is switching between
several programs to run more slowly, in direct proportion to the number of
programs it is running, but most programs spend much of their time waiting for
slow input/output devices to complete their tasks. If a program is waiting for
the user to click on the mouse or press a key on the keyboard, then it will not
take a “time slice” until the event it is waiting for has occurred. This frees
up time for other programs to execute so that many programs may be run
simultaneously without unacceptable speed loss.
**** Multiprocessing ****
Main article: Multiprocessing
Cray designed many supercomputers that used multiprocessing heavily.
Some computers are designed to distribute their work across several CPUs in a
multiprocessing configuration, a technique once employed only in large and
powerful machines such as supercomputers, mainframe_computers and servers.
Multiprocessor and multi-core (multiple CPUs on a single integrated circuit)
personal and laptop computers are now widely available, and are being
increasingly used in lower-end markets as a result.
Supercomputers in particular often have highly unique architectures that differ
significantly from the basic stored-program architecture and from general
purpose computers.[56] They often feature thousands of CPUs, customized high-
speed interconnects, and specialized computing hardware. Such designs tend to
be useful only for specialized tasks due to the large scale of program
organization required to successfully utilize most of the available resources
at once. Supercomputers usually see usage in large-scale simulation, graphics
rendering, and cryptography applications, as well as with other so-called
“embarrassingly_parallel” tasks.
**** Networking and the Internet ****
Main articles: Computer_networking and Internet
Visualization of a portion of the routes on the Internet
Computers have been used to coordinate information between multiple locations
since the 1950s. The U.S. military's SAGE system was the first large-scale
example of such a system, which led to a number of special-purpose commercial
systems such as Sabre.[57]
In the 1970s, computer engineers at research institutions throughout the United
States began to link their computers together using telecommunications
technology. The effort was funded by ARPA (now DARPA), and the computer_network
that resulted was called the ARPANET.[58] The technologies that made the
Arpanet possible spread and evolved.
In time, the network spread beyond academic and military institutions and
became known as the Internet. The emergence of networking involved a
redefinition of the nature and boundaries of the computer. Computer operating
systems and applications were modified to include the ability to define and
access the resources of other computers on the network, such as peripheral
devices, stored information, and the like, as extensions of the resources of an
individual computer. Initially these facilities were available primarily to
people working in high-tech environments, but in the 1990s the spread of
applications like e-mail and the World_Wide_Web, combined with the development
of cheap, fast networking technologies like Ethernet and ADSL saw computer
networking become almost ubiquitous. In fact, the number of computers that are
networked is growing phenomenally. A very large proportion of personal
computers regularly connect to the Internet to communicate and receive
information. “Wireless” networking, often utilizing mobile phone networks, has
meant networking is becoming increasingly ubiquitous even in mobile computing
environments.
**** Computer architecture paradigms ****
There are many types of computer_architectures:
    * Quantum_computer vs Chemical_computer
    * Scalar_processor vs Vector_processor
    * Non-Uniform_Memory_Access (NUMA) computers
    * Register_machine vs Stack_machine
    * Harvard_architecture vs von_Neumann_architecture
    * Cellular_architecture
Of all these abstract_machines, a quantum computer holds the most promise for
revolutionizing computing.[59]
Logic_gates are a common abstraction which can apply to most of the above
digital or analog paradigms.
The ability to store and execute lists of instructions called programs makes
computers extremely versatile, distinguishing them from calculators. The
Church–Turing_thesis is a mathematical statement of this versatility: any
computer with a minimum_capability_(being_Turing-complete) is, in principle,
capable of performing the same tasks that any other computer can perform.
Therefore any type of computer (netbook, supercomputer, cellular_automaton,
etc.) is able to perform the same computational tasks, given enough time and
storage capacity.
***** Misconceptions *****
Main articles: Human_computer and Harvard_Computers
Women as computers in NACA High Speed Flight Station "Computer Room"
A computer does not need to be electronic, nor even have a processor, nor RAM,
nor even a hard_disk. While popular usage of the word “computer” is synonymous
with a personal electronic computer, the modern[60] definition of a computer is
literally “A device that computes, especially a programmable [usually]
electronic machine that performs high-speed mathematical or logical operations
or that assembles, stores, correlates, or otherwise processes information.”[61]
Any device which processes information qualifies as a computer, especially if
the processing is purposeful.
**** Required technology ****
Main article: Unconventional_computing
Historically, computers evolved from mechanical_computers and eventually from
vacuum_tubes to transistors. However, conceptually computational systems as
flexible as a personal computer can be built out of almost anything. For
example, a computer can be made out of billiard balls (billiard_ball_computer);
an often quoted example.[citation_needed] More realistically, modern computers
are made out of transistors made of photolithographed semiconductors.
There is active research to make computers out of many promising new types of
technology, such as optical_computers, DNA_computers, neural_computers, and
quantum_computers. Most computers are universal, and are able to calculate any
computable_function, and are limited only by their memory capacity and
operating speed. However different designs of computers can give very different
performance for particular problems; for example quantum computers can
potentially break some modern encryption algorithms (by quantum_factoring) very
quickly.
***** Capabilities of computers (In general) *****
1.) Ability to perform certain logical and mathematical functions.
2.) Ability to store data and/or information.
3.) Ability to retrieve data and/or information.
4.) Ability to search data and/or information.
5.) Ability to compare data and/or information.
6.) Ability to sort data and/or information.
7.) Ability to control errors.
8.) Ability to check itself.
9.) Ability to perform a set of tasks with speed and accuracy.
10.) Ability to do a set of tasks repetitively.
11.) Ability to provide new time dimensions.
12.) Excellent substitute for writing instrument and paper.
***** Limitations of computers (In general) *****
1.) Dependence on prepared set of instructions.
2.) Inability to derive meanings from objects.
3.) Inability to generate data and/or information on its own.
4.) Cannot correct wrong instructions.
5.) Dependence on electricity.
6.) Dependence on human interventions.
7.) Inability to decide on its own.
8.) Not maintenance-free.
9.) Limited to the processing speed of its interconnected peripherals.
10.) Limited to the available amount of storage on primary data storage
devices.
11.) Limited to the available amount of storage on secondary data storage
devices.
12.) Not a long-term investment.
***** Further topics *****
    * Glossary_of_computers
**** Artificial intelligence ****
A computer will solve problems in exactly the way it is programmed to, without
regard to efficiency, alternative solutions, possible shortcuts, or possible
errors in the code. Computer programs that learn and adapt are part of the
emerging field of artificial_intelligence and machine_learning.
**** Hardware ****
Main articles: Computer_hardware and Personal_computer_hardware
The term hardware covers all of those parts of a computer that are tangible
objects. Circuits, displays, power supplies, cables, keyboards, printers and
mice are all hardware.
*** History of computing hardware ***
Main article: History_of_computing_hardware
                                                     Pascal's_calculator,
                          Calculators                Arithmometer, Difference
First generation                                     engine, Quevedo's
(mechanical/                                         analytical_machines
electromechanical)                                   Jacquard_loom, Analytical
                          Programmable devices       engine, IBM_ASCC/Harvard
                                                     Mark_I, Harvard_Mark_II,
                                                     IBM_SSEC, Z3
                                                     Atanasoff–Berry_Computer,
                          Calculators                IBM_604, UNIVAC_60, UNIVAC
                                                     120
                                                     Colossus, ENIAC,
Second generation (vacuum                            Manchester_Small-Scale
tubes)                                               Experimental_Machine,
                          Programmable_devices       EDSAC, Manchester_Mark_1,
                                                     Ferranti_Pegasus, Ferranti
                                                     Mercury, CSIRAC, EDVAC,
                                                     UNIVAC_I, IBM_701, IBM
                                                     702, IBM_650, Z22
Third generation          Mainframes                 IBM_7090, IBM_7080, IBM
(discrete transistors and                            System/360, BUNCH
SSI, MSI, LSI integrated  Minicomputer               PDP-8, PDP-11, IBM_System/
circuits)                                            32, IBM_System/36
                          Minicomputer               VAX, IBM_System_i
                          4-bit microcomputer        Intel_4004, Intel_4040
                                                     Intel_8008, Intel_8080,
                          8-bit microcomputer        Motorola_6800, Motorola
                                                     6809, MOS_Technology_6502,
                                                     Zilog_Z80
                          16-bit microcomputer       Intel_8088, Zilog_Z8000,
                                                     WDC_65816/65802
                          32-bit microcomputer       Intel_80386, Pentium,
Fourth generation (VLSI                              Motorola_68000, ARM
integrated circuits)                                 Alpha, MIPS, PA-RISC,
                          64-bit microcomputer[62]   PowerPC, SPARC, x86-64,
                                                     ARMv8-A
                          Embedded_computer          Intel_8048, Intel_8051
                                                     Desktop_computer, Home
                                                     computer, Laptop_computer,
                          Personal computer          Personal_digital_assistant
                                                     (PDA), Portable_computer,
                                                     Tablet_PC, Wearable
                                                     computer
                          Quantum_computer, Chemical
Theoretical/experimental  computer, DNA_computing,
                          Optical_computer,
                          Spintronics_based_computer
*** Other hardware topics ***
                                               Mouse, keyboard, joystick, image
                          Input                scanner, webcam, graphics
Peripheral_device (input/                      tablet, microphone
output)                   Output               Monitor, printer, loudspeaker
                                               Floppy_disk_drive, hard_disk
                          Both                 drive, optical_disc drive,
                                               teleprinter
                          Short range          RS-232, SCSI, PCI, USB
Computer_busses           Long range (computer Ethernet, ATM, FDDI
                          networking)
**** Software ****
Main article: Computer_software
Software refers to parts of the computer which do not have a material form,
such as programs, data, protocols, etc. When software is stored in hardware
that cannot easily be modified (such as BIOS ROM in an IBM_PC_compatible), it
is sometimes called “firmware.”
                                           UNIX_System_V, IBM_AIX, HP-UX,
                 Unix and BSD              Solaris (SunOS), IRIX, List_of
                                           BSD_operating_systems
                                           List_of_Linux_distributions,
                 GNU/Linux                 Comparison_of_Linux
                                           distributions
                                           Windows_95, Windows_98, Windows
                 Microsoft_Windows         NT, Windows_2000, Windows_Me,
Operating_system                           Windows_XP, Windows_Vista,
                                           Windows_7, Windows_8
                 DOS                       86-DOS (QDOS), IBM_PC_DOS, MS-
                                           DOS, DR-DOS, FreeDOS
                 Mac_OS                    Mac_OS_classic, Mac_OS_X
                 Embedded and real-time    List_of_embedded_operating
                                           systems
                 Experimental              Amoeba, Oberon/Bluebottle, Plan
                                           9_from_Bell_Labs
                 Multimedia                DirectX, OpenGL, OpenAL
Library          Programming library       C_standard_library, Standard
                                           Template_Library
Data             Protocol                  TCP/IP, Kermit, FTP, HTTP, SMTP
                 File_format               HTML, XML, JPEG, MPEG, PNG
                 Graphical_user_interface  Microsoft_Windows, GNOME, KDE,
User_interface   (WIMP)                    QNX_Photon, CDE, GEM, Aqua
                 Text-based_user_interface Command-line_interface, Text
                                           user_interface
                                           Word_processing, Desktop
                                           publishing, Presentation
                 Office_suite              program, Database_management
                                           system, Scheduling &amp; Time
                                           management, Spreadsheet,
                                           Accounting_software
                                           Browser, E-mail_client, Web
                 Internet Access           server, Mail_transfer_agent,
                                           Instant_messaging
                                           Computer-aided_design,
                                           Computer-aided_manufacturing,
                 Design and manufacturing  Plant management, Robotic
                                           manufacturing, Supply chain
                                           management
                                           Raster_graphics_editor, Vector
                                           graphics_editor, 3D_modeler,
                 Graphics                  Animation_editor, 3D_computer
                                           graphics, Video_editing, Image
                                           processing
                                           Digital_audio_editor, Audio
Application      Audio                     playback, Mixing, Audio
                                           synthesis, Computer_music
                                           Compiler, Assembler,
                                           Interpreter, Debugger, Text
                                           editor, Integrated_development
                 Software_engineering      environment, Software
                                           performance_analysis, Revision
                                           control, Software_configuration
                                           management
                 Educational               Edutainment, Educational_game,
                                           Serious_game, Flight_simulator
                                           Strategy, Arcade, Puzzle,
                                           Simulation, First-person
                 Games                     shooter, Platform, Massively
                                           multiplayer, Interactive
                                           fiction
                                           Artificial_intelligence,
                                           Antivirus_software, Malware
                 Misc                      scanner, Installer/Package
                                           management_systems, File
                                           manager
**** Languages ****
There are thousands of different programming languages—some intended to be
general purpose, others useful only for highly specialized applications.
                             Programming_languages
                                     Timeline_of_programming_languages, List_of
                                     programming_languages_by_category,
Lists of programming languages       Generational_list_of_programming
                                     languages, List_of_programming_languages,
                                     Non-English-based_programming_languages
Commonly used assembly_languages     ARM, MIPS, x86
Commonly used high-level_programming Ada, BASIC, C, C++, C#, COBOL, Fortran,
languages                            Java, Lisp, Pascal, Object_Pascal
Commonly used scripting_languages    Bourne_script, JavaScript, Python, Ruby,
                                     PHP, Perl
**** Professions and organizations ****
As the use of computers has spread throughout society, there are an increasing
number of careers involving computers.
                        Computer-related_professions
                 Electrical_engineering, Electronic_engineering, Computer
Hardware-related engineering, Telecommunications_engineering, Optical
                 engineering, Nanoengineering
                 Computer_science, Computer_engineering, Desktop_publishing,
Software-related Human–computer_interaction, Information technology,
                 Information_systems, Computational_science, Software
                 engineering, Video_game_industry, Web_design
The need for computers to work well together and to be able to exchange
information has spawned the need for many standards organizations, clubs and
societies of both a formal and informal nature.
                                Organizations
Standards groups                 ANSI, IEC, IEEE, IETF, ISO, W3C
Professional societies           ACM, AIS, IET, IFIP, BCS
Free/open_source_software groups Free_Software_Foundation, Mozilla_Foundation,
                                 Apache_Software_Foundation
***** Degradation *****
Rasberry_crazy_ants have been known to consume the insides of electrical wiring
in computers; preferring DC to AC currents. This behavior is not well
understood by scientists.[63]
